import os
import argparse  # ì¸ì ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€
import numpy as np
import torch
from rocket import Rocket
from policy import ActorCritic
import matplotlib.pyplot as plt
import utils
import glob
import cv2
import imageio 
import matplotlib
from IPython.display import HTML, display
import base64
import io

# [Headless ì„¤ì •] Colab ë“± ëª¨ë‹ˆí„°ê°€ ì—†ëŠ” í™˜ê²½ì—ì„œ Qt í”ŒëŸ¬ê·¸ì¸ ì—ëŸ¬ ë°©ì§€
os.environ["QT_QPA_PLATFORM"] = "offscreen"

# [GUI ë¬´ë ¥í™”] cv2.imshow í˜¸ì¶œ ì‹œ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•Šë„ë¡ ë¹ˆ í•¨ìˆ˜ë¡œ ëŒ€ì²´
cv2.imshow = lambda *args: None

# [Matplotlib ì„¤ì •] GUI ë°±ì—”ë“œ ëŒ€ì‹  Agg(ì´ë¯¸ì§€ ìƒì„±ìš©) ë°±ì—”ë“œ ì‚¬ìš©
matplotlib.use('Agg')   

# --- 1. GIF ì¬ìƒ ë° ì €ì¥ í—¬í¼ í•¨ìˆ˜ ---
def show_video(file_path):
    """ì €ì¥ëœ GIF íŒŒì¼ì„ ì½ì–´ Colab/Jupyter í™”ë©´ì— ì¶œë ¥"""
    if not os.path.exists(file_path):
        print("íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    with open(file_path, 'rb') as f:
        data = f.read()
    encoded = base64.b64encode(data).decode()

    # Colabì—ì„œ ë°”ë¡œ ë³´ì´ë„ë¡ HTML img íƒœê·¸ ì‚¬ìš©
    display(HTML(f'<img src="data:image/gif;base64,{encoded}" width="640" />'))

def save_video(frames, path, fps=30):
    """í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ë¥¼ GIF íŒŒì¼ë¡œ ì €ì¥ (ë¬´í•œ ë°˜ë³µ loop=0)"""
    imageio.mimsave(path, frames, fps=fps, loop=0)

# GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ CUDA, ì•„ë‹ˆë©´ CPU ì‚¬ìš©
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


if __name__ == '__main__':

    # --- 2. ì¸ì(Argument) ì„¤ì • ---
    parser = argparse.ArgumentParser(description="Rocket Recycling RL Training (PPO/A2C)")
    
    parser.add_argument('--task', type=str, default='landing', choices=['hover', 'landing'],
                        help="í•™ìŠµ ëª©í‘œ ì„¤ì •: 'hover'(í˜¸ë²„ë§) ë˜ëŠ” 'landing'(ì°©ë¥™). (ê¸°ë³¸ê°’: landing)")
    
    parser.add_argument('--max_m_episode', type=int, default=800000,
                        help="ì´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜. (ê¸°ë³¸ê°’: 800000)")
    
    parser.add_argument('--max_steps', type=int, default=800,
                        help="í•œ ì—í”¼ì†Œë“œ ë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜. (ê¸°ë³¸ê°’: 800)")
    
    parser.add_argument('--video_interval', type=int, default=8000,
                        help="GIF ì €ì¥ ë° ì‹œê°í™” ì£¼ê¸°(ì—í”¼ì†Œë“œ ë‹¨ìœ„). (ê¸°ë³¸ê°’: 50)")

    args = parser.parse_args()

    # ì¸ì ê°’ ë³€ìˆ˜ í• ë‹¹
    task = args.task
    max_m_episode = args.max_m_episode
    max_steps = args.max_steps
    video_interval = args.video_interval

    print(f"ğŸš€ Training Start! Task: {task}, Device: {device}")
    print(f"âš™ï¸  Settings: Episodes={max_m_episode}, MaxSteps={max_steps}, VideoInterval={video_interval}")

    # --- 3. í™˜ê²½ ë° ëª¨ë¸ ì´ˆê¸°í™” ---
    # ë¡œì¼“ í™˜ê²½ ìƒì„±
    env = Rocket(task=task, max_steps=max_steps)

    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í´ë” ìƒì„±
    ckpt_folder = os.path.join('./', task + '_ckpt')
    if not os.path.exists(ckpt_folder):
        os.mkdir(ckpt_folder)

    last_episode_id = 0
    REWARDS = []

    # Actor-Critic ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
    net = ActorCritic(input_dim=env.state_dims, output_dim=env.action_dims).to(device)

    # --- 4. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì´ì–´ì„œ í•™ìŠµí•˜ê¸°) ---
    ckpt_list = glob.glob(os.path.join(ckpt_folder, '*.pt'))
    if len(ckpt_list) > 0:
        ckpt_list.sort()
        ckpt_path = ckpt_list[-1]
        print(f"ğŸ”„ Loading checkpoint: {ckpt_path}")

        # weights_only=FalseëŠ” êµ¬ë²„ì „ PyTorch íŒŒì¼ ë¡œë“œ í˜¸í™˜ì„±ì„ ìœ„í•¨
        checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)

        net.load_state_dict(checkpoint['model_G_state_dict'])
        last_episode_id = int(checkpoint['episode_id'])
        REWARDS = list(map(float, checkpoint['REWARDS']))

    # --- 5. ë©”ì¸ í•™ìŠµ ë£¨í”„ ---
    for episode_id in range(last_episode_id, max_m_episode):

        # ì—í”¼ì†Œë“œ ì‹œì‘: ìƒíƒœ ì´ˆê¸°í™”
        state = env.reset()

        rewards, log_probs, values, masks = [], [], [], []
        
        # ì‹œê°í™”(GIF) ì €ì¥ ì—¬ë¶€ í™•ì¸
        is_video_episode = (episode_id % video_interval == 0)
        frames = [] 

        for step_id in range(max_steps):

            # í–‰ë™ ê²°ì • (Action)
            action, log_prob, value = net.get_action(state)
            
            # í™˜ê²½ì— í–‰ë™ ì ìš© (Step)
            state, reward, done, _ = env.step(action)

            # ë°ì´í„° ìˆ˜ì§‘
            rewards.append(reward)
            log_probs.append(log_prob)
            values.append(value)
            masks.append(1 - done)

            # --- ì´ë¯¸ì§€ ìº¡ì²˜ (GIF ìƒì„±ìš©) ---
            if is_video_episode:
                render_result = env.render()

                # íŠœí”Œ í˜•íƒœë¡œ ë°˜í™˜ë  ê²½ìš° ì´ë¯¸ì§€(ì²«ë²ˆì§¸ ìš”ì†Œ)ë§Œ ì¶”ì¶œ
                if isinstance(render_result, (tuple, list)):
                    img = render_result[0]
                else:
                    img = render_result

                # ìœ íš¨í•œ ì´ë¯¸ì§€ ë°ì´í„°ì¸ì§€ í™•ì¸ ë° ì „ì²˜ë¦¬
                if isinstance(img, np.ndarray):
                    # Float(0~1) íƒ€ì…ì„ Uint8(0~255)ë¡œ ë³€í™˜
                    if img.dtype != np.uint8:
                        if img.max() <= 1.5:
                            img = (img * 255).astype(np.uint8)
                        else:
                            img = img.astype(np.uint8)
                    
                    # í‘ë°±(2D)ì¼ ê²½ìš° RGB(3D)ë¡œ ì°¨ì› í™•ì¥
                    if len(img.shape) == 2:
                         img = np.stack((img,)*3, axis=-1)
                         
                    frames.append(img)

            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´ (ì¶”ë½, ì°©ë¥™, ë˜ëŠ” ìµœëŒ€ ìŠ¤í… ë„ë‹¬)
            if done or step_id == max_steps - 1:
                _, _, Qval = net.get_action(state)
                
                # ì •ì±… ì—…ë°ì´íŠ¸ (Policy Update)
                net.update_ac(
                    net, rewards, log_probs, values, masks, Qval, gamma=0.999
                )
                break

        # ì—í”¼ì†Œë“œë³„ ì´ ë³´ìƒ ê¸°ë¡
        episode_reward = float(np.sum(rewards))
        REWARDS.append(episode_reward)

        # ì§„í–‰ ìƒí™© ë¡œê·¸ (10 ì—í”¼ì†Œë“œ ë§ˆë‹¤)
        if episode_id % 10 == 0:
            print(f"episode id: {episode_id}, episode reward: {episode_reward:.3f}")

        # --- 6. GIF ì €ì¥ ë° ì¶œë ¥ ---
        if is_video_episode and len(frames) > 0:
            gif_filename = os.path.join(ckpt_folder, f"video_{episode_id:08d}.gif")
            print(f"ğŸ¥ Saving GIF to {gif_filename}...")
            
            save_video(frames, gif_filename, fps=30)
            
            print("--- Current Training GIF ---")
            show_video(gif_filename)


        # --- 7. ê²°ê³¼ ê·¸ë˜í”„ ë° ëª¨ë¸ ì €ì¥ ---
        if episode_id % args.video_interval == 0: 

            # ë³´ìƒ ê·¸ë˜í”„ ì €ì¥
            plt.figure()
            plt.plot(REWARDS)
            plt.plot(utils.moving_avg(REWARDS, N=50))
            plt.legend(['episode reward', 'moving avg'], loc=2)
            plt.xlabel('m episode')
            plt.ylabel('reward')
            plt.savefig(os.path.join(ckpt_folder, f"rewards_{episode_id:08d}.jpg"))
            plt.close()

            # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
            save_path = os.path.join(ckpt_folder, f"ckpt_{episode_id:08d}.pt")
            torch.save(
                {
                    'episode_id': int(episode_id),
                    'REWARDS': [float(r) for r in REWARDS],
                    'model_G_state_dict': net.state_dict(),
                },
                save_path
            )
            print(f"ğŸ’¾ Saved checkpoint: {save_path}")