import os
import argparse # μΈμ μ²λ¦¬λ¥Ό μ„ν•΄ μ¶”κ°€
import numpy as np
import torch
from rocket import Rocket
from policy import ActorCritic
import glob
import cv2
import imageio 
from IPython.display import HTML, display
import base64
import io

# [Headless μ„¤μ •] Colab λ“± λ¨λ‹ν„°κ°€ μ—†λ” ν™κ²½μ—μ„ Qt ν”λ¬κ·ΈμΈ μ—λ¬ λ°©μ§€
os.environ["QT_QPA_PLATFORM"] = "offscreen"

# [GUI λ¬΄λ ¥ν™”] cv2.imshow νΈμ¶ μ‹ μ—λ¬κ°€ λ‚μ§€ μ•λ„λ΅ λΉ ν•¨μλ΅ λ€μ²΄
cv2.imshow = lambda *args: None

# --- 1. GIF μ €μ¥ λ° μ¬μƒ ν—¬νΌ ν•¨μ ---
def show_video(file_path):
    """μ €μ¥λ GIF νμΌμ„ μ½μ–΄ Colab/Jupyter ν™”λ©΄μ— μ¶λ ¥"""
    if not os.path.exists(file_path):
        print("νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤.")
        return

    with open(file_path, 'rb') as f:
        data = f.read()
    encoded = base64.b64encode(data).decode()

    display(HTML(f'<img src="data:image/gif;base64,{encoded}" width="640" />'))

def save_video(frames, path, fps=30):
    """ν”„λ μ„ λ¦¬μ¤νΈλ¥Ό GIF νμΌλ΅ μ €μ¥ (λ¬΄ν• λ°λ³µ loop=0)"""
    imageio.mimsave(path, frames, fps=fps, loop=0)

# GPU μ‚¬μ© κ°€λ¥ μ‹ CUDA, μ•„λ‹λ©΄ CPU μ‚¬μ©
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

if __name__ == '__main__':

    # --- 2. μΈμ(Argument) μ„¤μ • ---
    parser = argparse.ArgumentParser(description="Rocket Recycling Inference & GIF Generation")
    
    parser.add_argument('--task', type=str, default='landing', choices=['hover', 'landing'],
                        help="μΈνΌλ°μ¤ λ©ν‘ μ„¤μ •: 'hover'(νΈλ²„λ§) λλ” 'landing'(μ°©λ¥™). (κΈ°λ³Έκ°’: landing)")
    
    parser.add_argument('--max_steps', type=int, default=800,
                        help="μµλ€ μΈνΌλ°μ¤ μ¤ν… μ. (κΈ°λ³Έκ°’: 800)")

    args = parser.parse_args()

    task = args.task
    max_steps = args.max_steps
    
    print(f"π€ Inference Start! Task: {task}, Device: {device}")

    # --- 3. μ²΄ν¬ν¬μΈνΈ λ° λ¨λΈ λ΅λ“ ---
    
    # μ²΄ν¬ν¬μΈνΈ ν΄λ” κ²½λ΅
    ckpt_folder = os.path.join('./', task + '_ckpt')
    
    # κ°€μ¥ μµμ‹  μ²΄ν¬ν¬μΈνΈ νμΌ(*.pt) μλ™ κ²€μƒ‰
    ckpt_list = glob.glob(os.path.join(ckpt_folder, '*.pt'))
    if not ckpt_list:
        print(f"β μ¤λ¥: '{ckpt_folder}' ν΄λ”μ— ν›λ ¨λ μ²΄ν¬ν¬μΈνΈ νμΌμ΄ μ—†μµλ‹λ‹¤.")
        print(f"   λ¨Όμ € ν•™μµ(train)μ„ μ§„ν–‰ν•κ±°λ‚ task μ„¤μ •μ„ ν™•μΈν•μ„Έμ”.")
        exit()
        
    ckpt_list.sort()
    ckpt_dir = ckpt_list[-1] # κ°€μ¥ λ§μ§€λ§‰ νμΌ μ„ νƒ
    
    print(f"β΅οΈ Loading checkpoint: {ckpt_dir}")

    # ν™κ²½ λ° λ¨λΈ μ΄κΈ°ν™”
    env = Rocket(task=task, max_steps=max_steps)
    net = ActorCritic(input_dim=env.state_dims, output_dim=env.action_dims).to(device)
    
    # κ°€μ¤‘μΉ λ΅λ“ (weights_only=Falseλ” κµ¬λ²„μ „ νΈν™μ„±μ©)
    checkpoint = torch.load(ckpt_dir, map_location=device, weights_only=False)
    net.load_state_dict(checkpoint['model_G_state_dict'])

    # --- 4. μΈνΌλ°μ¤ λ£¨ν”„ (GIF ν”„λ μ„ μμ§‘) ---
    
    state = env.reset()
    frames = [] 
    step_count = 0
    
    print("--- μΈνΌλ°μ¤ μ§„ν–‰ μ¤‘... ---")

    for step_id in range(max_steps):
        
        # ν–‰λ™ κ²°μ • (Action)
        action, log_prob, value = net.get_action(state)
        state, reward, done, _ = env.step(action)
        
        # λ λ”λ§ λ° μ΄λ―Έμ§€ μΊ΅μ²
        render_result = env.render()
        
        # νν”/λ¦¬μ¤νΈ μ²λ¦¬ (μ΄λ―Έμ§€λ§ μ¶”μ¶)
        if isinstance(render_result, (tuple, list)):
            img = render_result[0]
        else:
            img = render_result
        
        # μ΄λ―Έμ§€ μ ν¨μ„± κ²€μ‚¬ λ° μ „μ²λ¦¬
        if isinstance(img, np.ndarray):
            # Float -> Uint8 λ³€ν™
            if img.dtype != np.uint8:
                if img.max() <= 1.5:
                    img = (img * 255).astype(np.uint8)
                else:
                    img = img.astype(np.uint8)
            
            # ν‘λ°± -> RGB λ³€ν™
            if len(img.shape) == 2:
                 img = np.stack((img,)*3, axis=-1)
                 
            frames.append(img)
            
        step_count += 1
        
        # μΆ…λ£ μ΅°κ±΄ (ν™κ²½ μ™„λ£, μ¶”λ½, λλ” μµλ€ μ¤ν…)
        if done or env.already_crash or step_id == max_steps - 1:
            break
            
    # --- 5. κ²°κ³Ό μ €μ¥ λ° μ¶λ ¥ ---
    if frames:
        gif_filename = os.path.join(ckpt_folder, "inference_result.gif")
        
        print(f"β… μΈνΌλ°μ¤ μ™„λ£. μ΄ {step_count} μ¤ν….")
        print(f"π’Ύ GIF μ €μ¥ μ¤‘: {gif_filename}")
        save_video(frames, gif_filename, fps=30)
        
        print("--- μΈνΌλ°μ¤ κ²°κ³Ό GIF ---")
        show_video(gif_filename)
    else:
        print("β λ…Ήν™”λ ν”„λ μ„μ΄ μ—†μ–΄ GIFλ¥Ό μ €μ¥ν•μ§€ λ»ν–μµλ‹λ‹¤.")